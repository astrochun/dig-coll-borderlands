{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining template\n",
    "\n",
    "This notebook includes Python code for downloading scanned text of borderlands newspapers and performing word frequency text analyses on the newspapers.\n",
    "\n",
    "To start, follow directions in the [Setup](#Setup) section. Once you know which data you would like to use, there are several options listed below for different text analyses.\n",
    "\n",
    "The work is part of the project _Using Newspapers as Data for Collaborative Pedagogy: A Multidisciplinary Interrogation of the Borderlands in Undergraduate Classrooms_, funded in part by the Mellon Foundation through the [Collections as Data](https://collectionsasdata.github.io/part2whole/) program. More information about the project is available found at [https://libguides.library.arizona.edu/newspapers-as-data](https://libguides.library.arizona.edu/newspapers-as-data).\n",
    "\n",
    "This notebook and additional text mining lessons are available at [https://github.com/jcoliver/dig-coll-borderlands](https://github.com/jcoliver/dig-coll-borderlands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The first decision to make is whether you want to use a small, sample data set or the larger set of data. The latter option requires the files to be downloaded and can take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you _do not_ want to use the larger set of scanned text, you can use the data that are distributed with this notebook. Running the code block below will show you the data that are available if you do not want to download the larger data set (you do not need to take any extra steps to use the data below, they come with this Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>lccn</th>\n",
       "      <th>language</th>\n",
       "      <th>directory</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bisbee Daily Review</td>\n",
       "      <td>Bisbee</td>\n",
       "      <td>sn84024827</td>\n",
       "      <td>English</td>\n",
       "      <td>bisbee-daily-review</td>\n",
       "      <td>1917-01-02</td>\n",
       "      <td>1919-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Border Vidette</td>\n",
       "      <td>Nogales</td>\n",
       "      <td>sn96060796</td>\n",
       "      <td>English</td>\n",
       "      <td>border-vidette</td>\n",
       "      <td>1917-01-06</td>\n",
       "      <td>1919-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El Tucsonense</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>sn95060694</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>el-tucsonense</td>\n",
       "      <td>1917-01-03</td>\n",
       "      <td>1919-12-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name    place        lccn language            directory  \\\n",
       "0  Bisbee Daily Review   Bisbee  sn84024827  English  bisbee-daily-review   \n",
       "1       Border Vidette  Nogales  sn96060796  English       border-vidette   \n",
       "2        El Tucsonense   Tucson  sn95060694  Spanish        el-tucsonense   \n",
       "\n",
       "        start         end  \n",
       "0  1917-01-02  1919-12-31  \n",
       "1  1917-01-06  1919-12-27  \n",
       "2  1917-01-03  1919-12-30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run to display table with newspaper information\n",
    "import pandas\n",
    "titles = pandas.read_csv(\"data/sample/sample-titles.csv\")\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use the entire suite of scanned borderlands newspapers, you will need to first download the files from the University of Arizona Data Repository. Executing the code block below with do this for you (if you just want to try things out with a smaller data set, do not run this block and just jump ahead). Note the data are contained in an archive around 1.5GB and include hundreds of thousands of files. Both the downloading and file extraction steps may take a little while, so now might be a good time to refill your beverage. When the download and extraction process is complete, a table showing the available data will be printed below the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries necessary for download & extraction\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "# Location of the file on the UA Data Repository\n",
    "url = \"https://arizona.figshare.com/ndownloader/files/24201092\"\n",
    "\n",
    "# Download the file & write it to disk\n",
    "zip_filename = \"fulldata.zip\"\n",
    "download = requests.get(url, allow_redirects = True)\n",
    "with open(zip_filename, \"wb\") as z:\n",
    "    z.write(download.content)\n",
    "\n",
    "# Set the destination for the data files\n",
    "destination = \"data/complete/\"\n",
    "\n",
    "# Make sure the destination directory exists\n",
    "if(not(os.path.isdir(destination))):\n",
    "    os.makedirs(destination)\n",
    "\n",
    "# Extract files to destination directory\n",
    "with zipfile.ZipFile(zip_filename, \"r\") as zipdata:\n",
    "    zipdata.extractall(destination)\n",
    "    \n",
    "# No need for that zipfile, so we can remove it\n",
    "os.remove(zip_filename)\n",
    "\n",
    "# Finally, display the available titles for this full data set\n",
    "full_titles = pandas.read_csv(\"data/complete/full-titles.csv\")\n",
    "display(full_titles.sort_values(by=['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency analyses\n",
    "\n",
    "For the newspaper(s) of choice, there are a variety of analyses that can be performed with code below.\n",
    "\n",
    "+ [Investigate word frequency over time for a single word in a single newspaper](#Single-word-in-one-newspaper)\n",
    "+ [Investigate word frequency over time in a pair of newspapers](#Single-word-in-two-newspapers)\n",
    "+ [Investigate frequencies of two words over time for a single newspaper](#Two-words-in-one-newspaper)\n",
    "\n",
    "In all analyses, the code below has example values for newspapers, words, and dates. You can change these as necessary for your specific question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word in one newspaper\n",
    "\n",
    "The code block below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, one newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word in two newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, two newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two words in one newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for two sets of words in one newspaper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
