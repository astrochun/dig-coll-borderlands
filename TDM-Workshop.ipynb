{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Jupyter Notebooks\n",
    "This lesson will introduce the Jupyter Notebook interface. We will use the interface to run and write, yes, write, some Python code for text data analysis.\n",
    "\n",
    "By the end of this lesson, learners should be able to:\n",
    "1. Explain the difference between markdown and code blocks in Jupyter Notebooks\n",
    "2. Execute pre-written Python code to analyze newspaper text\n",
    "3. Modify Python code to change the settings of the analysis\n",
    "\n",
    "And just an aside, all the code for this fun stuff is availble on GitHub at [https://github.com/jcoliver/dig-coll-borderlands](https://github.com/jcoliver/dig-coll-borderlands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this Jupyter Notebook thing?\n",
    "\n",
    "Jupyter Notebooks are effectively made up of \"cells\". We can start by thinking of each cell being equivalent to a paragraph on a page. There is an order in which paragraphs and cells appear, and that order matters. In Jupyter Notebooks, the cells come in two flavors and a single notebook (like the one we are working in now) with have both types of cells. \n",
    "+ The first is called \"markdown\", which is text, like you are reading now. We can use some syntax in the text to format the cells in particular ways. For example, we can create italic text by using the underscore symbol (\"\\_\") at the beginning and ending of the text we want to italicize. So when we write \"\\_italic\\_\" in a markdown block, it will show up as _italic_.\n",
    "+ The second kind of cell is a \"code\" cell, that contains computer code in a language like Python or R. This is where the fun comes in.\n",
    "\n",
    "\n",
    "So let's try this out. Click your cursor in the box below on the word \"Data\" and run the cell. You can run the cell by holding down the Control (Ctrl) key and press Enter (on a Windows machine). If you are on a Mac, you hold down the Command (Cmd) key instead of the Control key and press Enter. You can also click the button labeled \"Run\" at the top of the screen, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collections as Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we are going to to today is work with some text files on some text data mining questions.\n",
    "### Looking at one file\n",
    "We will start with a single text file.\n",
    "\n",
    "The code block below sets up the name of the file we want to use. There are a couple of important pieces we need to provide:\n",
    "1. The title of the paper, here in a machine-readable form\n",
    "2. The date of the paper, with four digit year, two digit month, and two digit day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"border-vidette\"\n",
    "year = \"1919\"\n",
    "month = \"01\"\n",
    "day = \"04\"\n",
    "\n",
    "# We also need to indicate where the data are stored (i.e. which folder are they in)\n",
    "# If one is working on their own computer, the data folder will be:\n",
    "datapath = \"data/\"\n",
    "\n",
    "# If one is working on a cloud computer (like Atmosphere), the path will be different:\n",
    "# datapath = \"../../data/\"\n",
    "\n",
    "# We stitch all those pieces of information together, along with the folder \n",
    "# information about where data for an entire day's paper is located\n",
    "filename = datapath + title + \"/volumes/\" + year + month + day + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run that code block, nothing will visibly happen. We haven't asked Python to print anything, and there were no errors (yay!). But we might want to check our work to make sure the file name was specified correctly. So we can use our `print` command again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we did not enter a phrase enclosed with quotation marks, but instead provided the word `filename`. But it didn't print \"filename\". Rather it printed the value stored in the _variable_ called `filename`. If you can think back to high school algebra, this is a similar sort of concept - we use a variable, in this case `filename` to store information, much like we would use the variable \"x\" in a mathematical equation.\n",
    "\n",
    "At this point, we are ready to read the file and do some work with it. Before we do so, we will need to tell Python about some additional programs to use. By default, Python does not come with text data mining tools, so those are installed separately and we make them available for use using the `import` command. Run the code block below to load those packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional packages\n",
    "import pandas  # for data tables\n",
    "import os      # for file navigation\n",
    "import re      # for pattern matching in filenames\n",
    "import nltk    # for text data mining\n",
    "from nltk.corpus import stopwords           # for stopword corpora\n",
    "from nltk.tokenize import RegexpTokenizer   # for splitting data into individual words\n",
    "import digcol as dc\n",
    "#import digcol.cleantext as dc               # for automated text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download the stopwords. There are _a lot_ of recognized stopwords (i.e. \"y\", \"a\", \"el\", \"la\", \"del\", \"que\", etc.), so we don't want to enter them by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords for several languages\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read in the data and start looking around. The code block below will read in all the text from the day's paper and clean it up. By \"clean it up\", the `CleanText` does the following:\n",
    "1. Removes stop words (here we use English stop words)\n",
    "2. Removes words that are one character long\n",
    "3. Removes punctuation\n",
    "4. \"Tokenizes\" the data. In this case, that means is breaks the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = dc.CleanText(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, nothing visibly happened, so we can check our work by looking at the first 20 words. Run the code block below (remember click the box and press Ctrl+Enter or Cmd+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdata.clean_list[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this list of words to calculate relative frequency of each word. Relative frequencies in this case are in regards to the length of the issue. We count the number of times a word occurs, and divide that by the total number of words in the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with all the words\n",
    "word_table = pandas.Series(newdata.clean_list)\n",
    "\n",
    "# Calculate relative frequency of each word\n",
    "word_freqs = word_table.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our work, we look at the first 10 rows of the `word_freqs` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_freqs.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no big surprise that \"Arizona\" and \"Nogales\" are the most frequent words, given that the paper was printed in Nogales, Arizona.\n",
    "## Beyond counting\n",
    "Now we can broaden our focus to look at trends over time. We are going to look a multiple years of papers to track how the frequency of influenza coverages changes over time. We will stick with _The Border Vidette_ but instead of looking at a single issue, we will look at all the issues 1917-1919.\n",
    "\n",
    "Here's where it gets fun. We could try to do this file-by-file, but that would be extremely tedious. So we are going to give Python a little bit of information and let the computer look at every single file. But first we need to tell Python _which_ files to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern that will match the dates of interest. In this case, papers from \n",
    "# 1917, 1918, and 1919\n",
    "date_pattern = re.compile(\"((1917)|(1918)|(1919))([0-9]{4})*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what the hell does that even mean? What we have in the code block above is something called \"regular expressions\". Regular expressions is a very powerful pattern matching tool with a very terrible name. What we are saying above is that we want any files that:\n",
    "+ start with \"1917\", \"1918\", or \"1919\",\n",
    "+ followed by four digits, i.e. the two-digit month and two-digit day, so May 1 is represented as \"0501\"\n",
    "    + `[0-9]` matches any single digit between 0 and 9\n",
    "    + `{4}` means that there are four consecutive digits\n",
    "+ and end with anything (the asterisk \"*\" is a wild card, matching and letters, numbers, or symbols)\n",
    "So be sure you run the code block above (Ctrl-Enter or Cmd-Enter) before moving on. You will know that the code block has been run when you see a number show up in between the square brackets to the left of the code block (`In [ ]:`).\n",
    "\n",
    "We are now ready to start reading in the files. We need to start by listing _all_ the _Border Vidette_ issues, then filtering only those that are in the date range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the Border Vidette files and store in bv_volumes variable\n",
    "volume_path = datapath + title + \"/volumes/\"\n",
    "bv_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "bv_volumes = list(filter(date_pattern.match, bv_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "bv_volumes.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another opportunity for a reality check, so we ask Python to print out the first five files that we will ask Python to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bv_volumes[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know which files to look at, so we can instruct Python to do so. We are ultimately going to want to create a table that has two columns of data:\n",
    "1. The date the paper was published. Thankfully, this is stored in the filename: the file 19170113.txt is the paper that was published on January 13, 1917.\n",
    "2. The relative frequency of the words we are interested in for that date's paper\n",
    "\n",
    "We will start by creating a table that will hold that information. We need to extract dates for each paper. While the filenames have that information, we need to convert it to an actual date, in the form of YYYY-MM-DD, so the date for 19170113.txt is 1917-01-13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in bv_volumes:\n",
    "    one_date = str(one_file[0:4]) + \"-\" + str(one_file[4:6]) + \"-\" + str(one_file[6:8])\n",
    "    dates.append(one_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait. No. What?\n",
    "\n",
    "So there's a bit going on there up above.\n",
    "+ `dates = []` creates an empty list of dates. There is nothing in there to start with.\n",
    "+ `for one_file in bv_volumes:` says we are going to cycle through all of the files that are listed in the `bv_volumes` variable; each cycle, the value of `one_file` changes to the next value. If you look at the output we created above when running `print(bv_volumes[1:5])`, the first time through the cycle, `one_file` will have the value '19170113.txt'. The second time, `one_file` will have the value '19170120.txt'.\n",
    "+ `one_date = str(one_file[0:4]) + str(one_file[4:6]) + str(one_file[6:8])` is creating a - no, we just need to run some code to explain this one.\n",
    "\n",
    "Let us do a little test, seeing what this code does on an example of `one_file`. We start by pulling out the very first value in `bv_volumes`, as if it was the first cycle through the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_file = bv_volumes[1]\n",
    "print(one_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Looking good. What we are doing with the `one_date` line is pulling out parts of that filename using indexing. An index is basically an address for each letter. For the first That is, we pull out the 0<sup>th</sup> through 3<sup>rd</sup> part of the file name via `one_file[0:4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first four characters\n",
    "print(one_file[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at characters 5 and 6\n",
    "print(one_file[4:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the entireity of the filename, these are the indexes of the filenames:\n",
    "\n",
    "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |\n",
    "|---|---|---|---|---|---|---|---|---|---|----|----|\n",
    "| 1 | 9 | 1 | 7 | 0 | 1 | 1 | 3 | . | t | x  | t  |\n",
    "\n",
    "If we run the piece of code that stitches all the pieces together,\n",
    "\n",
    "And in the last part of the cycle above, `dates.append[one_date]`, we add that one date to our list of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(one_file[0:4]) + \"-\" + str(one_file[4:6]) + \"-\" + str(one_file[6:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the date formatted like we want. Now that we have all those dates, we can set up our table (finally!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add those dates to a data frame\n",
    "flu_table = pandas.DataFrame(dates, columns = [\"Date\"])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "flu_table[\"Frequency\"] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a reality check, let's look at the first six rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_table.head(n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Python to cycle over every file, calculate the relative frequency of flu and influenza, and store the result in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_words = ['flu', 'influenza']\n",
    "\n",
    "for issue in bv_volumes:\n",
    "    issue_text = dc.CleanText(volume_path + issue)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values for flu or influenza\n",
    "    flu_freqs = word_freqs.filter(flu_words)\n",
    "    \n",
    "    # Get the total frequency for flu and influenza\n",
    "    total_flu_freq = flu_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + \"-\" + str(issue[4:6]) + \"-\" + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    flu_table.loc[flu_table[\"Date\"] == issue_date, \"Frequency\"] = total_flu_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look again at the first six rows of our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_table.head(n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...they are all still zeros. But maybe that isn't surprising, since the influenza pandemic did not really get going until late 1918, and we are just looking at the early 1917 issues here. When doing this sort of quality assurance, we can pick a paper that we _know_ will have at least some occurrences of influenza. The November 16 issue from 1918 had at least some mention of influenza, so we can look at the corresponding row for that date via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a 1918 date where we know there should be non-zero values\n",
    "flu_table.loc[flu_table[\"Date\"] == \"1918-11-16\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We have our data ready to go. All we need to do now is graph it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more package is needed for plotting\n",
    "import plotly.express as px\n",
    "flu_figure = px.line(flu_table, x = \"Date\", y = \"Frequency\")\n",
    "flu_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph should show a peak in relative frequency of flu/influenza over the winter of 1918-1919.\n",
    "\n",
    "## Your turn\n",
    "\n",
    "The code block below includes all the code necessary to make a graph like the one above, but you get to determine what it shows. You'll need to provide:\n",
    "\n",
    "1. The title of the newspaper you want to analyze\n",
    "2. The range of years to include (note, consult the table below for available date ranges for each paper)\n",
    "3. The words you are interested in including\n",
    "\n",
    "Run the code block below to display a table with relevant information regarding available titles and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to display table with newspaper information\n",
    "titles = pandas.read_csv(\"data/titles.csv\")\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the first four variables in the code block below then run the code block. Your graph should appear below the block once it has finished running. You'll know it finished when the asterisk in the square brackets is replaced by a number (e.g. `In [*]` -> `In [31]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = \"el-sol\" \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1943', '1944', '1945']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "my_words = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = \"spanish\"\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = \")|(\"\n",
    "years = years.join(year_list)\n",
    "pattern = \"((\" + years + \"))([0-9]{4})*\"\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + \"/volumes/\"\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + \"-\" + str(one_file[4:6]) + \"-\" + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = [\"Date\"])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table[\"Frequency\"] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    my_freqs = word_freqs.filter(my_words)\n",
    "    \n",
    "    # Get the total frequency for words of interest\n",
    "    total_my_freq = my_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + \"-\" + str(issue[4:6]) + \"-\" + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table[\"Date\"] == issue_date, \"Frequency\"] = total_my_freq\n",
    "    \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = \"Date\", y = \"Frequency\")\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than one line?\n",
    "If you want to plot more than one line on a plot, you can use the code below. The code below is set up for plotting two lines for one title of one language, but it could be extended to multiple lines, titles, and languages (but probably not today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = \"el-sol\" \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1943', '1944', '1945']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "words_1_name = \"German\"\n",
    "words_2 = ['japona', 'japón']\n",
    "words_2_name = \"Japan\"\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = \"spanish\"\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = \")|(\"\n",
    "years = years.join(year_list)\n",
    "pattern = \"((\" + years + \"))([0-9]{4})*\"\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + \"/volumes/\"\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + \"-\" + str(one_file[4:6]) + \"-\" + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = [\"Date\"])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "#results_table[\"Frequency\"] = 0.0\n",
    "results_table[words_1_name] = 0.0\n",
    "results_table[words_2_name] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    words_1_freqs = word_freqs.filter(words_1)\n",
    "    words_2_freqs = word_freqs.filter(words_2)\n",
    "\n",
    "    # Get the total frequency for words of interest\n",
    "    total_words_1 = words_1_freqs.sum()\n",
    "    total_words_2 = words_2_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + \"-\" + str(issue[4:6]) + \"-\" + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table[\"Date\"] == issue_date, words_1_name] = total_words_1\n",
    "    results_table.loc[results_table[\"Date\"] == issue_date, words_2_name] = total_words_2\n",
    "    \n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = \"Date\", value_vars = [words_1_name, words_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':\"Words\"}, inplace = True)\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_melt, x = \"Date\" , y = \"Frequency\" , color = \"Words\")\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew. We're done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
