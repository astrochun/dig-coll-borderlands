{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Jupyter Notebooks\n",
    "This lesson will introduce the Jupyter Notebook interface. We will use the interface to run and write, yes, write, some Python code for text data analysis.\n",
    "\n",
    "By the end of this lesson, learners should be able to:\n",
    "1. Explain the difference between markdown and code blocks in Jupyter Notebooks\n",
    "2. Execute pre-written Python code to analyze newspaper text\n",
    "3. Modify Python code to change the settings of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this Jupyter Notebook thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebooks are effectively made up of \"cells\". We can start by thinking of each cell being equivalent to a paragraph on a page. There is an order in which paragraphs and cells appear, and that order matters. In Jupyter Notebooks, the cells come in two flavors and a single notebook (like the one we are working in now) with have both types of cells. \n",
    "+ The first is called \"markdown\", which is text, like you are reading now. We can use some syntax in the text to format the cells in particular ways. For example, we can create italic text by using the underscore symbol (\"\\_\") at the beginning and ending of the text we want to italicize. So when we write \"\\_italic\\_\" in a markdown block, it will show up as _italic_.\n",
    "+ The second kind of cell is a \"code\" cell, that contains computer code in a language like Python or R. This is where the fun comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do some markdown stuff?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections as Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Collections as Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is Python then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add brief explantion of python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**talk about what we are going to do**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may need to happen first, to get stopwords downloaded to all learners' home folder?\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff and run it\n",
    "from scipy import stats\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data and roadmap where we want to go. Maybe whiteboard the entire process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/border-vidette/volumes/19190104.txt\n"
     ]
    }
   ],
   "source": [
    "# download data and do a little reality check\n",
    "title_1 = \"border-vidette\"\n",
    "year = \"1919\"\n",
    "month = \"01\"\n",
    "day = \"04\"\n",
    "filename = \"data/\" + title_1 + \"/volumes/\" + year + month + day + \".txt\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rVrritoria' Library State House\n",
      "NTY-SEVENTH YEAR.\n",
      "NOGALES, SANTA CRUZ COUNTY. ARIZONA, JANUARY 4, 1919.\n",
      "No. 1.\n",
      "I\n",
      "- ..;..n.\n",
      "ANGLO-AMERICAN\n",
      "COAT POCKET FLASHLIGHTS\n",
      "FLAT OPENING\n",
      "OR\n",
      "CIGARETTE CASE STYLE\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# open the file so we can read the text from it\n",
    "file = open(filename, \"r\")\n",
    "# read the file and store in variable issue_text\n",
    "issue_text = file.read()\n",
    "# close the file after reading in the text\n",
    "file.close()\n",
    "# print the first 200 characters of the text\n",
    "print(issue_text[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['library', 'state', 'house', 'nty', 'seventh', 'year', 'nogales', 'santa', 'cruz']\n"
     ]
    }
   ],
   "source": [
    "# convert everything to lower case (otherwise \"House\" and \"house\" are considered different words)\n",
    "issue_text = issue_text.lower()\n",
    "\n",
    "# remove punctuation and \"tokenize\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "issue_text = tokenizer.tokenize(issue_text)\n",
    "\n",
    "# look at first ten words in the output\n",
    "print(issue_text[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some word counting before removing stopwords\n",
    "\n",
    "Could leave line 4 with an error in it (value_count instead of value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the    570\n",
      "of     417\n",
      "and    279\n",
      "a      223\n",
      "to     195\n",
      "in     174\n",
      "at     107\n",
      "is      96\n",
      "for     89\n",
      "i       77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make a table with words in it\n",
    "word_table = pandas.Series(issue_text)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removal of stopwords and maybe single characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona    71\n",
      "nogales    62\n",
      "j          55\n",
      "w          34\n",
      "r          34\n",
      "f          33\n",
      "state      27\n",
      "e          25\n",
      "p          23\n",
      "year       23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load the appropriate corpora\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords\n",
    "filtered_words = []\n",
    "for word in issue_text:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "# Recalculate word counts\n",
    "word_table = pandas.Series(filtered_words)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona     71\n",
      "nogales     62\n",
      "state       27\n",
      "year        23\n",
      "one         21\n",
      "business    21\n",
      "ed          21\n",
      "day         20\n",
      "co          20\n",
      "people      20\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove stop words AND single letter words\n",
    "filtered_words = []\n",
    "for word in issue_text:\n",
    "    if word not in stop_words:\n",
    "        if len(word) > 1:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "# Recalculate word counts\n",
    "word_table = pandas.Series(filtered_words)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona     0.011356\n",
      "nogales     0.009917\n",
      "state       0.004319\n",
      "year        0.003679\n",
      "one         0.003359\n",
      "business    0.003359\n",
      "ed          0.003359\n",
      "day         0.003199\n",
      "co          0.003199\n",
      "people      0.003199\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# instead of counts, return relative frequencies\n",
    "word_freqs = word_table.value_counts(normalize = True)\n",
    "# print the top ten most frequent words\n",
    "print(word_freqs.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond counting\n",
    "There is a lot more we can do more than just count words. For example, we can look for specific words and see how their frequency changes over time. Given the publication dates of the newspapers we are looking at and current events, we can look at the frequency of the words \"flu\" and \"influenza\". And see how this frequency is changing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flu          4\n",
      "influenza    3\n",
      "dtype: int64\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Search for \"flu\" and \"influenza\" in one volume of interest (middle of influenza pandemic)\n",
    "influenza_words = ['flu', 'influenza']\n",
    "influenza_freq = word_counts.filter(influenza_words)\n",
    "print(influenza_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get relative frequency for all volumes in a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all volumes in a single year, calculating frequency of flu and influenza for each volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparative analysis - two papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all volumes of second title in a single year, calculating rel. freq. of flu & influenza\n",
    "# Second title is Bisbee Daily Review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test comparing the two papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
