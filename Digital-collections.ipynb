{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Jupyter Notebooks\n",
    "This lesson will introduce the Jupyter Notebook interface. We will use the interface to run and write, yes, write, some Python code for text data analysis.\n",
    "\n",
    "By the end of this lesson, learners should be able to:\n",
    "1. Explain the difference between markdown and code blocks in Jupyter Notebooks\n",
    "2. Execute pre-written Python code to analyze newspaper text\n",
    "3. Modify Python code to change the settings of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this Jupyter Notebook thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebooks are effectively made up of \"cells\". We can start by thinking of each cell being equivalent to a paragraph on a page. There is an order in which paragraphs and cells appear, and that order matters. In Jupyter Notebooks, the cells come in two flavors and a single notebook (like the one we are working in now) with have both types of cells. \n",
    "+ The first is called \"markdown\", which is text, like you are reading now. We can use some syntax in the text to format the cells in particular ways. For example, we can create italic text by using the underscore symbol (\"\\_\") at the beginning and ending of the text we want to italicize. So when we write \"\\_italic\\_\" in a markdown block, it will show up as _italic_.\n",
    "+ The second kind of cell is a \"code\" cell, that contains computer code in a language like Python or R. This is where the fun comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do some markdown stuff?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections as Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Collections as Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is Python then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add brief explantion of python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**talk about what we are going to do**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may need to happen first, to get stopwords downloaded to all learners' home folder?\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff and run it\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data and roadmap where we want to go. Maybe whiteboard the entire process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/border-vidette/volumes/19190104.txt\n"
     ]
    }
   ],
   "source": [
    "# download data and do a little reality check\n",
    "title_1 = \"border-vidette\"\n",
    "year = \"1919\"\n",
    "month = \"01\"\n",
    "day = \"04\"\n",
    "filename = \"data/\" + title_1 + \"/volumes/\" + year + month + day + \".txt\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rVrritoria' Library State House\n",
      "NTY-SEVENTH YEAR.\n",
      "NOGALES, SANTA CRUZ COUNTY. ARIZONA, JANUARY 4, 1919.\n",
      "No. 1.\n",
      "I\n",
      "- ..;..n.\n",
      "ANGLO-AMERICAN\n",
      "COAT POCKET FLASHLIGHTS\n",
      "FLAT OPENING\n",
      "OR\n",
      "CIGARETTE CASE STYLE\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# open the file so we can read the text from it\n",
    "file = open(filename, \"r\")\n",
    "# read the file and store in variable issue_text\n",
    "issue_text = file.read()\n",
    "# close the file after reading in the text\n",
    "file.close()\n",
    "# print the first 200 characters of the text\n",
    "print(issue_text[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['library', 'state', 'house', 'nty', 'seventh', 'year', 'nogales', 'santa', 'cruz']\n"
     ]
    }
   ],
   "source": [
    "# convert everything to lower case (otherwise \"House\" and \"house\" are considered different words)\n",
    "issue_text = issue_text.lower()\n",
    "\n",
    "# remove punctuation and \"tokenize\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "issue_text = tokenizer.tokenize(issue_text)\n",
    "\n",
    "# look at first ten words in the output\n",
    "print(issue_text[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some word counting before removing stopwords\n",
    "\n",
    "Could leave line 4 with an error in it (value_count instead of value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the    570\n",
      "of     417\n",
      "and    279\n",
      "a      223\n",
      "to     195\n",
      "in     174\n",
      "at     107\n",
      "is      96\n",
      "for     89\n",
      "i       77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make a table with words in it\n",
    "word_table = pandas.Series(issue_text)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removal of stopwords and maybe single characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona    71\n",
      "nogales    62\n",
      "j          55\n",
      "w          34\n",
      "r          34\n",
      "f          33\n",
      "state      27\n",
      "e          25\n",
      "year       23\n",
      "p          23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load the appropriate corpora\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords\n",
    "filtered_words = []\n",
    "for word in issue_text:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "# Recalculate word counts\n",
    "word_table = pandas.Series(filtered_words)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona     71\n",
      "nogales     62\n",
      "state       27\n",
      "year        23\n",
      "one         21\n",
      "ed          21\n",
      "business    21\n",
      "co          20\n",
      "people      20\n",
      "day         20\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove stop words AND single letter words\n",
    "filtered_words = []\n",
    "for word in issue_text:\n",
    "    if word not in stop_words:\n",
    "        if len(word) > 1:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "# Recalculate word counts\n",
    "word_table = pandas.Series(filtered_words)\n",
    "# count how many times each word occurs\n",
    "word_counts = word_table.value_counts()\n",
    "# print the top ten most common words and their respective counts\n",
    "print(word_counts.head(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona     0.011356\n",
      "nogales     0.009917\n",
      "state       0.004319\n",
      "year        0.003679\n",
      "one         0.003359\n",
      "ed          0.003359\n",
      "business    0.003359\n",
      "co          0.003199\n",
      "people      0.003199\n",
      "day         0.003199\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# instead of counts, return relative frequencies\n",
    "word_freqs = word_table.value_counts(normalize = True)\n",
    "# print the top ten most frequent words\n",
    "print(word_freqs.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond counting\n",
    "There is a lot more we can do more than just count words. For example, we can look for specific words and see how their frequency changes over time. Given the publication dates of the newspapers we are looking at and current events, we can look at the frequency of the words \"flu\" and \"influenza\". And see how this frequency is changing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flu          4\n",
      "influenza    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Search for \"flu\" and \"influenza\" in one volume of interest (middle of influenza pandemic)\n",
    "influenza_words = ['flu', 'influenza']\n",
    "influenza_freq = word_counts.filter(influenza_words)\n",
    "print(influenza_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get relative frequency for all volumes in a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all volumes in a single year, calculating frequency of flu and influenza for each volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative analysis\n",
    "We can also make comparisons between different titles. Here we are going to compare the Bisbee Daily Review and the Border Vidette to see if there is a difference in the coverage of the [mine strike of 1917](https://en.wikipedia.org/wiki/Bisbee_Deportation#Strike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Border Vidette:\n",
      "['19170915.txt', '19170804.txt', '19170811.txt', '19171027.txt', '19170901.txt', '19170818.txt', '19170707.txt', '19170728.txt', '19171013.txt', '19170714.txt', '19170825.txt', '19170908.txt', '19170630.txt', '19171006.txt', '19170922.txt', '19170609.txt', '19170602.txt', '19170929.txt', '19171020.txt', '19170616.txt', '19170623.txt']\n",
      "Bisbee Daily Review:\n",
      "['19170701.txt', '19170915.txt', '19170921.txt', '19170817.txt', '19170804.txt', '19171004.txt', '19170811.txt', '19170815.txt', '19170824.txt']\n"
     ]
    }
   ],
   "source": [
    "# Loop over all volumes of second title in a single year, calculating rel. freq. of 'strike' and 'strikes'\n",
    "# Second title is Bisbee Daily Review?\n",
    "# Look at 'deportation' and 'strike', in June - October of 1917 for Bisbee Daily Review\n",
    "\n",
    "# Get volumes for each title 191706** through 191710**\n",
    "# bv = Border Vidette\n",
    "# bdr = Bisbee Daily Review\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Create a pattern that will match papers June - October, 1917\n",
    "date_pattern = re.compile(\"1917((06)|(07)|(08)|(09)|(10))([0-9]{2})*\")\n",
    "\n",
    "# List all the Border Vidette files\n",
    "bv_volumes = os.listdir(\"data/border-vidette/volumes\")\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "bv_volumes = list(filter(date_pattern.match, bv_volumes))\n",
    "\n",
    "# Do a little reality check to make sure we only see files in desired date range.\n",
    "print(\"Border Vidette:\")\n",
    "print(bv_volumes)\n",
    "\n",
    "# Download and filter files for bisbee-daily-review (like above)\n",
    "bdr_volumes = os.listdir(\"data/bisbee-daily-review/volumes\")\n",
    "bdr_volumes = list(filter(date_pattern.match, bdr_volumes))\n",
    "print(\"Bisbee Daily Review:\")\n",
    "print(bdr_volumes[0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate relative frequency of \"strike\" for each issue for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.00033101621979476995, 0.0, 0.0, 0.0, 0.0, 0.0004702931493964571, 0.0, 0.0001719986240110079, 0.0006744225257123588, 0.0, 0.0, 0.0001563232765358762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001857700167193015, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# A tool we can re-use to break up words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# A list of the words of interest\n",
    "strike_words = ['strike', 'strikers', 'strikes']\n",
    "\n",
    "# For all Border Vidette volumes, calculate the relative frequency of 'strike'\n",
    "bv_strike_freq = []\n",
    "bv_file_locations = (\"data/border-vidette/volumes/\")\n",
    "for one_issue in bv_volumes:\n",
    "    # Read in text\n",
    "    issue_location = bv_file_locations + one_issue\n",
    "    issue_file = open(issue_location, \"r\")\n",
    "    issue_text = issue_file.read()\n",
    "    issue_file.close()\n",
    "    \n",
    "    # convert to lower case\n",
    "    issue_text = issue_text.lower()\n",
    "    issue_text = tokenizer.tokenize(issue_text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    filtered_words = []\n",
    "    for word in issue_text:\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 1:\n",
    "                filtered_words.append(word)\n",
    "\n",
    "    # make a table with words in it\n",
    "    word_table = pandas.Series(filtered_words)\n",
    "    # calculate relative frequencies\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "\n",
    "    # pull out only values for 'strike', 'striker', or 'strikes'\n",
    "    strike_freqs = word_freqs.filter(strike_words)\n",
    "    \n",
    "    # add those frequencies to our list of values for Border Vidette\n",
    "    bv_strike_freq.append(strike_freqs.sum())\n",
    "\n",
    "print(bv_strike_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0018363939899833056, 0.00013264358668258389, 0.00036768568126904083, 0.0008564393533882881, 0.001246555570135153, 0.000501336898395722, 0.0003936078091789341, 0.00040257648953301127, 0.0006528835690968445, 0.00016173378618793466, 0.0009913070001525087, 0.0004993065187239945, 0.0006412124744972312, 0.0009314781393724166, 0.0002230400356864057, 0.0017501093818363647, 0.0005339598462195643, 0.0009174311926605505, 0.0013420362349783445, 0.0007610832751950276, 0.0005429864253393665, 0.0008604864616796695, 0.00011014428901861439, 0.0009204470742932282, 7.042253521126761e-05, 0.00012219710392863688, 0.0006555793250983369, 0.0007020231029421151, 6.477522995206633e-05, 0.0008223684210526315, 0.0004931395584950541, 0.00039691990156386444, 0.00041564088627425906, 0.00047024400438894405, 0.00043896673982779, 0.0004172876304023845, 0.0, 0.0006618620385350787, 0.0029359953024075164, 0.0005826487210860572, 0.0034113447044823485, 5.990893841361131e-05, 0.0011521369397048335, 0.00038031076822775184, 0.0016028209648982208, 0.0001682274435036169, 0.00017375188231205839, 0.003963287442636629, 0.0002756035718222908, 0.003482223464167491, 0.00014585764294049007, 0.0007113499841922226, 0.0004005264061337758, 0.0006089459698848539, 0.0023553299492385786, 0.0009958408997715424, 0.002026266416510319, 0.00314486907773296, 0.00042835724994645533, 0.00042633015006821284, 0.002056986134389761, 0.0003190606853423521, 0.0003306565895134625, 0.00040950040950040947, 0.0009032943676939426, 0.0009784735812133072, 0.0007957559681697613, 0.0004407875404055245, 0.000826570483919447, 0.0012309685779073533, 0.0002919537545252832, 0.0007710640684144118, 0.00028213292491233725, 0.00028260562385191464, 5.4068667207353336e-05, 0.0015106881184379485, 0.00037895192724122996, 0.001911102891313729, 0.000163363101720758, 0.0009185713513805587, 0.00010626992561105207, 0.0004921259842519685, 0.0004195888029730864, 0.0012987012987012987, 0.0020639834881320948, 0.0009493070058857035, 0.000502108857200241, 0.00019159535061949164, 0.0006932008549477212, 0.00028068205739948076, 0.00019716643663915726, 0.00024286581663630845, 0.000594035879767138, 0.00041992546323027665, 0.0009779951100244498, 0.0008767734736170891, 0.0003065791894046232, 0.0007240474251063445, 0.0003389064618165386, 0.0006942034015966678, 0.0003779697624190065, 0.0016116483272201844, 0.0015514566454059645, 0.0008512570228704386, 0.0007619876993414249, 0.00027649769585253456, 0.001253317605426128, 0.000883538571980783, 0.0007272727272727273, 0.00038847523470378765, 0.0021310421811047593, 0.004035448646937807, 0.0008590604026845639, 0.00035313001605136434, 0.00023315458148752622, 0.00033893709327548807, 6.855419208884623e-05, 0.0, 0.0005108173076923077, 0.002354788069073783, 0.0022373218428902884, 0.0008341953351796856, 0.0006672826198542244]\n"
     ]
    }
   ],
   "source": [
    "# Do same calculations for Bisbee daily review\n",
    "bdr_strike_freq = []\n",
    "bdr_file_locations = (\"data/bisbee-daily-review/volumes/\")\n",
    "for one_issue in bdr_volumes:\n",
    "    # Read in text\n",
    "    issue_location = bdr_file_locations + one_issue\n",
    "    issue_file = open(issue_location, \"r\")\n",
    "    issue_text = issue_file.read()\n",
    "    issue_file.close()\n",
    "    \n",
    "    # convert to lower case\n",
    "    issue_text = issue_text.lower()\n",
    "    issue_text = tokenizer.tokenize(issue_text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    filtered_words = []\n",
    "    for word in issue_text:\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 1:\n",
    "                filtered_words.append(word)\n",
    "\n",
    "    # make a table with words in it\n",
    "    word_table = pandas.Series(filtered_words)\n",
    "    # calculate relative frequencies\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "\n",
    "    # pull out only values for 'strike', 'striker', or 'strikes'\n",
    "    strike_freqs = word_freqs.filter(strike_words)\n",
    "    \n",
    "    # add those frequencies to our list of values for Border Vidette\n",
    "    bdr_strike_freq.append(strike_freqs.sum())\n",
    "\n",
    "print(bdr_strike_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000095 Border Vidette\n",
      "0.000840 Bisbee Daily Review\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean relative frequencies for each of the papers\n",
    "from statistics import mean\n",
    "# Border Vidette\n",
    "bv_mean = mean(bv_strike_freq)\n",
    "# print the mean, using format instead of str to avoid scientific notation\n",
    "print(format(bv_mean, 'f') + \" Border Vidette\")\n",
    "# Bisbee Daily Review\n",
    "bdr_mean = mean(bdr_strike_freq)\n",
    "print(format(bdr_mean, 'f') + \" Bisbee Daily Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But are these _significantly_ different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = -8.8751\n",
      "p = 4.137e-15\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# run the test\n",
    "compare_strike = stats.ttest_ind(bv_strike_freq, bdr_strike_freq, equal_var = False)\n",
    "\n",
    "# extract values of interest, Student's t and the p-value\n",
    "t_value = compare_strike[0]\n",
    "p_value = compare_strike[1]\n",
    "\n",
    "# print test statistics\n",
    "print(\"t = \" + format(t_value, '.3f')) # normal formatting\n",
    "print(\"p = \" + format(p_value, '.3e')) # scientific notation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
